{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c0ce85-501a-422a-8a5d-07fbddf4b59a",
   "metadata": {},
   "source": [
    "# QUESTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31791874-b748-41f9-be36-5c384b070764",
   "metadata": {},
   "source": [
    "Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd37e8-cbc7-4f84-a0d8-a4ed3aa198a3",
   "metadata": {},
   "source": [
    "Overfitting means having low bias and high variance whereas underfitting means high bias and high variance in our model result. They can cause accuracy issues and make our modeling unreliable and therefore output may not be true.\n",
    "They can be mitigated by ensuring we don't overfit/underfit while training the dataset using many strategies to ensure a right mix of data is used, proper handling of outliers and not removing excess data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e6ec8-cade-492b-926b-8515822f3d25",
   "metadata": {},
   "source": [
    "# QUESTION 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbc398-c888-412a-89b5-8f4eee1647c9",
   "metadata": {},
   "source": [
    "Here we will discuss possible options to prevent overfitting, which helps improve the model performance.\n",
    "Train with more data. ...\n",
    "Data augmentation. ...\n",
    "Addition of noise to the input data. ...\n",
    "Feature selection. ...\n",
    "Cross-validation. ...\n",
    "Simplify data. ...\n",
    "Regularization. ...\n",
    "Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d30ad-ac07-48d1-a8b4-3c90b043aa45",
   "metadata": {},
   "source": [
    "# QUESTION 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c593a-865a-4a9c-a77d-af1d6396ecea",
   "metadata": {},
   "source": [
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it inflexible in learning from the dataset. Simple learners tend to have less variance in their predictions but more bias towards wrong outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2c3608-c93d-4c32-82de-ff225d69396c",
   "metadata": {},
   "source": [
    "# QUESTION 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5184587-16fe-428d-ab68-63c9bc8d9489",
   "metadata": {},
   "source": [
    "t helps optimize the error in our model and keeps it as low as possible. An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data. In this, both the bias and variance should be low so as to prevent overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88619dd5-650c-43a1-811d-207f18cfb9fc",
   "metadata": {},
   "source": [
    "Bias variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53fdbbb-5727-49ea-bb0d-0ff9c30e343f",
   "metadata": {},
   "source": [
    "# QUESTION 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19319972-2a08-4ad6-bff5-c4cbff6b1715",
   "metadata": {},
   "source": [
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "\n",
    "\n",
    "Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). Your model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6764c06d-6cfe-4b71-aef7-c6de0e5125a2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Overfitting can be identified by checking validation metrics such as accuracy and loss. The validation metrics usually increase until a point where they stagnate or start declining when the model is affected by overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07209672-ef86-4381-8c70-16b050ef570f",
   "metadata": {},
   "source": [
    "# QUESTION 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c667f52-dfe2-4cf2-aa26-e4e1f6336a22",
   "metadata": {},
   "source": [
    "Also called “error due to squared biasExternal link:open_in_new,” bias is the amount that a model’s prediction differs from the target value, compared to the training data. Bias error results from simplifying the assumptions used in a model so the target functions are easier to approximate. Bias can be introduced by model selection. Data scientists conduct resampling to repeat the model building process and derive the average of prediction values. Resampling data is the process of extracting new samples from a data set in order to get more accurate results. There are a variety of ways to resample data including:\n",
    "\n",
    "K fold resampling, in which a given data set is split into a K number of sections, or folds, where each fold is used as a testing set.\n",
    "Bootstrapping, which involves iteratively resampling a dataset with replacement.\n",
    "Resampling can affect bias. If the average prediction values are significantly different from the true value based on the sample data, the model has a high level of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9bff2b-6462-4bd2-82a7-30998703de96",
   "metadata": {},
   "source": [
    "Variance indicates how much the estimate of the target function will alter if different training data were usedExternal link:open_in_new. In other words, variance describes how much a random variable differs from its expected value. Variance is based on a single training set. Variance measures the inconsistency of different predictions using different training sets — it’s not a measure of overall accuracy.\n",
    "\n",
    "Variance can lead to overfitting, in which small fluctuations in the training set are magnified. A model with high-level variance may reflect random noise in the training data set instead of the target function. The model should be able to identify the underlying connections between the input data and variables of the output.\n",
    "\n",
    "A model with low variance means sampled data is close to where the model predicted it would be. A model with high variance will result in significant changes to the projections of the target function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44286cb9-e106-4697-88ec-6dde79afd15d",
   "metadata": {},
   "source": [
    "High bias and high variance is underfitted models. They fail to establish relations even in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c6069b-e524-401f-ad0d-be7460199a7a",
   "metadata": {},
   "source": [
    "# QUESTION 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5533248c-b8c3-48ef-9bb5-f16fe8a37161",
   "metadata": {},
   "source": [
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea5af7-cbc3-42de-824b-39c9010a2d71",
   "metadata": {},
   "source": [
    "L1 Regularization technique is also known as LASSO or Least Absolute Shrinkage and Selection Operator. In this, the penalty term added to the cost function is the summation of absolute values of the coefficients. Since the absolute value of the coefficients is used, it can reduce the coefficient to 0 and such features may completely get discarded in LASSO. Thus, we can say, LASSO helps in Regularization as well as Feature Selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd88f3c-ae5a-4aeb-b0bf-bda5a20d9041",
   "metadata": {},
   "source": [
    "L2 Regularization technique is also known as Ridge. In this, the penalty term added to the cost function is the summation of the squared value of coefficients. Unlike the LASSO term, the Ridge term uses squared values of the coefficient and can reduce the coefficient value near to 0 but not exactly 0. Ridge distributes the coefficient value across all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734187a7-8712-490f-8864-7997026a999b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
